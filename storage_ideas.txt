- Cache shouldn't be replicated between nodes, they might not have the same access pattern.
    The option to replicate cache should be exposed with a REPLICATE_CACHE environment variable.
    Should metadata be replicated? Queries for cache existance should likely consult 
    other nodes first (controlled by environment variable - CONSULT_PEERS_FIRST: bool), once 
    retrieved, only querying node needs to store that info. 
- Partition metadata like file attributes should be in a relational structure while extended attributes 
    can be in a NoSQL one but ideally it would be another structure which consolidates them or just NoSQL 
    with some required fields, storage space requirements might grow but performance likely won't be impacted. 
    POSIX-ish compatible with extended attribute availability.
- The cache should likely include file lists or maybe that can be in partition metadata, either way, 
    need a messaging system where nodes can discard updates that don't pertain to them... or PubSub 
    which ensures only required nodes get sent the message. Additionally, this needs to support messages 
    for cache/metadata invalidation if a file or file listing has changed.
- Sharded cache behaviour should also be available when only keeping a cache in connected nodes. 
    It could just be considered one big cache with the file being cached by cumulative latency and 
    link speed to other nodes weighed by number of directly connected nodes to avoid indirect 
    transfers (when nodes proxying requests becomes a thing anyway) as well as highest amount of available 
    memory. Exposed with environment variables for toggle and configuration of mode.
- The above should be transparent to higher-level structures. Maybe a wrapper around StorageClient...
- 

Essentially, it's going to boil down to a tree of decisions as follows:
a) Cached or not, completely transparently by wrapping StorageClient
b) Cache type: sharded, replicated and eventually, intelligent
c) Whether to persist cache locally and whether to have a pre-built one in cloud storage...
    Hash being used to revalidate local caches.
d) Whether to consult cluster members' cache if the local doesn't have it.
e) Whether to aggregate records into more efficient, larger files on storage servers
f) Handling a distributed messaging system for information about cache invalidation as files are modified
    Likely by checking file hashes.
g) Keeping file metadata and extended attributes in NoSQL model. Some as required fields.
h) Delegation of hashing to S3 using ETags or storage servers for own first party solution
i) Schemas likely a file within the filesystem for having transparent caching. 
    Personalised validation functions may become a thing too.
j) Ability to open files as streams and mount using FUSE for VMs and applications
k) Ability to self-host the storage server in a distributed manner too.
l) Caching should have the ability to do chunks of the file only too... 
    no need to get the entire file if not needed.
m) Ranking algorithms for best nodes when proxying requests from other nodes and doing sharded caching:
    - Cumulative latency to all nodes
    - Latency to internet
    - Bandwidth
    - Compute and available memory
n) Ability to select above per storage client, partition and database if needed with sensible defaults. 
o) Update scripts for new database versions.
p) Distributed object locking with lock extension capability using messaging. 
    Centralised yet HA locking ability when using first party server.
u) Cache levels, i.e. SSD, disk, in-memory etc.
y) Asynchronous syncing to storage server
q) Computed fields and views for databases
w) Schema validation for YAML environment files using JSONSchema since it's all just a dictionary in-memory

Ideas for implementation:
- Asterisks mean default
- Classes implementing Cache interface (options for type passed as Dict[str, Union[cls, str]])
    Type specification as more specifc and explicit-only level overrides defaults. 
    Set per instance, partition type and partition. 
- Another class wrapping StorageClient for record aggregation
- All classes that act as a storage middleware can implement the StorageClient interface and call 
    the super method since they would be wrapping the actual implementation like MinioClient.
    That includes record aggregation, caching, 
- A lot of those wrappers need access to distributed information. 
    I suppose they can all inherit from Distributed class, no harm in redundancy.


Operation mechanics:
- Underlying storage operates in bytes
- Storage client operates in Objects (caching here by container)
- Partitions (databases) operate in JSON